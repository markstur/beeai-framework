---
title: "Agents"
icon: "robot"
---

## Overview

An AI agent is a system built on large language models (LLMs) that can solve complex tasks through structured reasoning and autonomous actions. Unlike basic chatbots, agents can:
- Perform multi-step reasoning
- Use tools to interact with external systems
- Maintain context across interactions
- Adapt based on feedback

These capabilities make them ideal for planning, research, analysis, and complex execution.

<Tip>
Dive deeper into the concepts behind AI agents in this [research article](https://research.ibm.com/blog/what-are-ai-agents-llm) from IBM.
</Tip>

<Note>
Explore the implementation in the [beeai_framework/agents directory](https://github.com/i-am-bee/beeai-framework/tree/python/beeai_framework/agents).
</Note>

## Agent Types

BeeAI Framework provides several agent implementations for different use cases:

### ReAct Agent

The ReActAgent implements the ReAct ([Reasoning and Acting](https://arxiv.org/abs/2210.03629)) pattern, which structures agent behavior into a cyclical process of reasoning, action, and observation.

This pattern allows agents to reason about a task, take actions using tools, observe results, and continue reasoning until reaching a conclusion.

Let's see how a ReActAgent approaches a simple question:

**Input prompt:** "What is the current weather in Las Vegas?"

**First iteration:**

```
thought: I need to retrieve the current weather in Las Vegas. I can use the OpenMeteo function to get the current weather forecast for a location.
tool_name: OpenMeteo
tool_input: {"location": {"name": "Las Vegas"}, "start_date": "2024-10-17", "end_date": "2024-10-17", "temperature_unit": "celsius"}
```

**Second iteration:**

```
thought: I have the current weather in Las Vegas in Celsius.
final_answer: The current weather in Las Vegas is 20.5簞C with an apparent temperature of 18.3簞C.
```

<Note>
During execution, the agent emits partial updates as it generates each line, followed by complete updates. Updates follow a strict order: first all partial updates for "thought," then a complete "thought" update, then moving to the next component.
</Note>

<CodeGroup>

{/* <!-- embedme python/examples/agents/react.py --> */}
```py Python
```

{/* <!-- embedme typescript/examples/agents/react.ts --> */}
```ts TypeScript
```

</CodeGroup>

### Tool Calling Agent

The ToolCallingAgent is optimized for scenarios where tool usage is the primary focus. It handles tool calls more efficiently and can execute multiple tools in parallel.

<CodeGroup>

{/* <!-- embedme python/examples/agents/toolCalling/agent.py --> */}
```py Python
```

{/* <!-- embedme typescript/examples/agents/toolCalling/agent.ts --> */}
```ts TypeScript
```

</CodeGroup>

### Remote Agent

The爹emoteAgent疳s a client-side abstraction for communicating with remote agents over API endpoints. It provides an asynchronous interface for sending prompts, receiving responses, and managing agent state. This allows agents to run on remote servers, containers, or cloud environments, rather than locally.

RemoteAgent is useful for building multi-agent systems by interacting with multiple agents through different API URLs. Each agent can handle a distinct task, and you can coordinate them within a single application. This makes it easy to compose modular, distributed agent systems, regardless of the framework or runtime each agent is built on.

<CodeGroup>

{/* <!-- embedme python/examples/agents/experimental/remote.py --> */}
```py Python [expandable]
import asyncio
import sys
import traceback

from beeai_framework.agents.experimental.remote import RemoteAgent
from beeai_framework.errors import FrameworkError
from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory
from examples.helpers.io import ConsoleReader


async def main() -> None:
    reader = ConsoleReader()

    agent = RemoteAgent(agent_name="chat", url="http://127.0.0.1:8333/api/v1/acp/", memory=UnconstrainedMemory())
    for prompt in reader:
        # Run the agent and observe events
        response = await agent.run(prompt).on(
            "update",
            lambda data, event: (reader.write("Agent  (debug) : ", data)),
        )

        reader.write("Agent  : ", response.result.text)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())
```

{/* <!-- embedme typescript/examples/agents/experimental/remote.ts --> */}
```ts TypeScript [expandable]
import "dotenv/config.js";
import { RemoteAgent } from "beeai-framework/agents/experimental/remote/agent";
import { createConsoleReader } from "examples/helpers/io.js";
import { FrameworkError } from "beeai-framework/errors";
import { TokenMemory } from "beeai-framework/memory/tokenMemory";

const agentName = "chat";

const instance = new RemoteAgent({
  url: "http://127.0.0.1:8333/api/v1/acp",
  agentName,
  memory: new TokenMemory(),
});

const reader = createConsoleReader();

try {
  for await (const { prompt } of reader) {
    const result = await instance.run({ input: prompt }).observe((emitter) => {
      emitter.on("update", (data) => {
        reader.write(`Agent (received progress)  : `, JSON.stringify(data.value, null, 2));
      });
      emitter.on("error", (data) => {
        reader.write(`Agent (error)  : `, data.message);
      });
    });

    reader.write(`Agent (${agentName})  : `, result.result.text);
  }
} catch (error) {
  reader.write("Agent (error)  ", FrameworkError.ensure(error).dump());
}
```

</CodeGroup>

### Custom Agent

For advanced use cases, you can create your own agent implementation by extending the `BaseAgent` class.

<CodeGroup>

{/* <!-- embedme python/examples/agents/custom_agent.py --> */}
```py Python [expandable]
import asyncio
import sys
import traceback

from pydantic import BaseModel, Field, InstanceOf

from beeai_framework.adapters.ollama import OllamaChatModel
from beeai_framework.agents import AgentMeta, BaseAgent, BaseAgentRunOptions
from beeai_framework.backend import AnyMessage, AssistantMessage, ChatModel, SystemMessage, UserMessage
from beeai_framework.context import Run, RunContext
from beeai_framework.emitter import Emitter
from beeai_framework.errors import FrameworkError
from beeai_framework.memory import BaseMemory, UnconstrainedMemory


class State(BaseModel):
    thought: str
    final_answer: str


class RunInput(BaseModel):
    message: InstanceOf[AnyMessage]


class CustomAgentRunOptions(BaseAgentRunOptions):
    max_retries: int | None = None


class CustomAgentRunOutput(BaseModel):
    message: InstanceOf[AnyMessage]
    state: State


class CustomAgent(BaseAgent[CustomAgentRunOutput]):
    memory: BaseMemory | None = None

    def __init__(self, llm: ChatModel, memory: BaseMemory) -> None:
        super().__init__()
        self.model = llm
        self.memory = memory

    def _create_emitter(self) -> Emitter:
        return Emitter.root().child(
            namespace=["agent", "custom"],
            creator=self,
        )

    def run(
        self,
        run_input: RunInput,
        options: CustomAgentRunOptions | None = None,
    ) -> Run[CustomAgentRunOutput]:
        async def handler(context: RunContext) -> CustomAgentRunOutput:
            class CustomSchema(BaseModel):
                thought: str = Field(description="Describe your thought process before coming with a final answer")
                final_answer: str = Field(
                    description="Here you should provide concise answer to the original question."
                )

            response = await self.model.create_structure(
                schema=CustomSchema,
                messages=[
                    SystemMessage("You are a helpful assistant. Always use JSON format for your responses."),
                    *(self.memory.messages if self.memory is not None else []),
                    run_input.message,
                ],
                max_retries=options.max_retries if options else None,
                abort_signal=context.signal,
            )

            result = AssistantMessage(response.object["final_answer"])
            await self.memory.add(result) if self.memory else None

            return CustomAgentRunOutput(
                message=result,
                state=State(thought=response.object["thought"], final_answer=response.object["final_answer"]),
            )

        return self._to_run(
            handler, signal=options.signal if options else None, run_params={"input": run_input, "options": options}
        )

    @property
    def meta(self) -> AgentMeta:
        return AgentMeta(
            name="CustomAgent",
            description="Custom Agent is a simple LLM agent.",
            tools=[],
        )


async def main() -> None:
    agent = CustomAgent(
        llm=OllamaChatModel("granite3.1-dense:8b"),
        memory=UnconstrainedMemory(),
    )

    response = await agent.run(RunInput(message=UserMessage("Why is the sky blue?")))
    print(response.state)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())
```

{/* <!-- embedme typescript/examples/agents/custom_agent.ts --> */}
```ts TypeScript [expandable]
```

</CodeGroup>

## Customizing Agent Behavior

You can customize your agent's behavior in five ways:

### 1. Setting Execution Policy

Control how the agent runs by configuring retries, timeouts, and iteration limits.

<CodeGroup>

```py Python
response = await agent.run(
     prompt=prompt,
     execution=AgentExecutionConfig(max_retries_per_step=3, total_max_retries=10, max_iterations=20),
).on("*", process_agent_events, EmitterOptions(match_nested=False))
```

```ts TypeScript
for await (const { prompt } of reader) {
  const response = await agent
    .run(
      { prompt },
      {
        execution: {
          maxRetriesPerStep: 3,
          totalMaxRetries: 10,
          maxIterations: 20,
        },
      },
    )
```

</CodeGroup>

<Tip>
  The default is zero retries and no timeout. For complex tasks, increasing the max_iterations is recommended.
</Tip>

### 2. Overriding Prompt Templates

Customize how the agent formats prompts, including the system prompt that defines its behavior.

The agent uses several templates that you can override:

1. **System Prompt** - Defines the agent's behavior and capabilities
2. **User Prompt** - Formats the user's input
3. **Tool Error** - Handles tool execution errors
4. **Tool Input Error** - Handles validation errors
5. **Tool No Result Error** - Handles empty results
6. **Tool Not Found Error** - Handles references to missing tools
7. **Invalid Schema Error** - Handles parsing errors

<CodeGroup>

{/* <!-- embedme python/examples/templates/system_prompt.py --> */}
```py Python
```

{/* <!-- embedme typescript/examples/templates/system_prompt.ts --> */}
```ts TypeScript
```

</CodeGroup>

### 3. Adding Tools

Enhance your agent's capabilities by providing it with tools to interact with external systems.

<CodeGroup>

```py Python
agent = ReActAgent(
    llm=llm,
    tools=[DuckDuckGoSearchTool(), OpenMeteoTool()],
    memory=UnconstrainedMemory()
)
```

```ts TypeScript
const agent = new ReActAgent({
  llm,
  memory: new TokenMemory(),
  tools: [new DuckDuckGoSearchTool(), new OpenMeteoTool()],
});
```

</CodeGroup>

**Available tools include:**

- Search tools (`DuckDuckGoSearchTool`)
- Weather tools (`OpenMeteoTool`)
- Knowledge tools (`LangChainWikipediaTool`)
- And many more in the `beeai_framework.tools` module

### 4. Configuring Memory

Memory allows your agent to maintain context across multiple interactions.

Several memory types are available for different use cases:
- UnconstrainedMemory - For unlimited storage
- SlidingMemory - For keeping only the most recent messages
- TokenMemory - For managing token limits
- SummarizeMemory - For summarizing previous conversations

<CodeGroup>

```py Python
agent = ReActAgent(
    llm=llm,
    tools=[DuckDuckGoSearchTool(), OpenMeteoTool()],
    memory=UnconstrainedMemory()
)
```

```ts TypeScript
const agent = new ReActAgent({
  llm,
  memory: new TokenMemory(),
  tools: [new DuckDuckGoSearchTool(), new OpenMeteoTool()],
});
```

</CodeGroup>

<CodeGroup>

{/* <!-- embedme python/examples/memory/agent_memory.py --> */}
```py Python [expandable]
import asyncio
import sys
import traceback

from beeai_framework.agents import AgentExecutionConfig
from beeai_framework.agents.react import ReActAgent
from beeai_framework.backend import AssistantMessage, ChatModel, UserMessage
from beeai_framework.errors import FrameworkError
from beeai_framework.memory import UnconstrainedMemory

# Initialize the memory and LLM
memory = UnconstrainedMemory()


def create_agent() -> ReActAgent:
    llm = ChatModel.from_name("ollama:granite3.1-dense:8b")

    # Initialize the agent
    agent = ReActAgent(llm=llm, memory=memory, tools=[])

    return agent


async def main() -> None:
    # Create user message
    user_input = "Hello world!"
    user_message = UserMessage(user_input)

    # Await adding user message to memory
    await memory.add(user_message)
    print("Added user message to memory")

    # Create agent
    agent = create_agent()

    response = await agent.run(
        prompt=user_input,
        execution=AgentExecutionConfig(max_retries_per_step=3, total_max_retries=10, max_iterations=20),
    )
    print(f"Received response: {response}")

    # Create and store assistant's response
    assistant_message = AssistantMessage(response.result.text)

    # Await adding assistant message to memory
    await memory.add(assistant_message)
    print("Added assistant message to memory")

    # Print results
    print(f"\nMessages in memory: {len(agent.memory.messages)}")

    if len(agent.memory.messages) >= 1:
        user_msg = agent.memory.messages[0]
        print(f"User: {user_msg.text}")

    if len(agent.memory.messages) >= 2:
        agent_msg = agent.memory.messages[1]
        print(f"Agent: {agent_msg.text}")
    else:
        print("No agent message found in memory")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())
```

{/* <!-- embedme typescript/examples/memory/agentMemory.ts --> */}
```ts TypeScript [expandable]
```

</CodeGroup>

### 5. Event Observation

Monitor the agent's execution by observing events it emits. This allows you to track its reasoning process, handle errors, or implement custom logging.

<CodeGroup>

```py Python
def update_callback(data: Any, event: EventMeta) -> None:
    print(f"Agent({data.update.key})  : ", data.update.parsed_value)

def on_update(emitter: Emitter) -> None:
    emitter.on("update", update_callback)

output: BeeRunOutput = await agent.run("What's the current weather in Las Vegas?").observe(on_update)
```

```ts TypeScript
const response = await agent
  .run({ prompt: "What's the current weather in Las Vegas?" })
  .observe((emitter) => {
    emitter.on("update", async ({ data, update, meta }) => {
      console.log(`Agent (${update.key})  : `, update.value);
    });
  });
```

</CodeGroup>

## Agent Workflows

For complex applications, you can create multi-agent workflows where specialized agents collaborate.

<CodeGroup>

{/* <!-- embedme python/examples/workflows/multi_agents.py --> */}
```py Python [expandable]
import asyncio
import sys
import traceback

from beeai_framework.backend import ChatModel
from beeai_framework.emitter import EmitterOptions
from beeai_framework.errors import FrameworkError
from beeai_framework.tools.search import WikipediaTool
from beeai_framework.tools.weather import OpenMeteoTool
from beeai_framework.workflows.agent import AgentWorkflow, AgentWorkflowInput
from examples.helpers.io import ConsoleReader


async def main() -> None:
    llm = ChatModel.from_name("ollama:llama3.1")
    workflow = AgentWorkflow(name="Smart assistant")

    workflow.add_agent(
        name="Researcher",
        role="A diligent researcher.",
        instructions="You look up and provide information about a specific topic.",
        tools=[WikipediaTool()],
        llm=llm,
    )

    workflow.add_agent(
        name="WeatherForecaster",
        role="A weather reporter.",
        instructions="You provide detailed weather reports.",
        tools=[OpenMeteoTool()],
        llm=llm,
    )

    workflow.add_agent(
        name="DataSynthesizer",
        role="A meticulous and creative data synthesizer",
        instructions="You can combine disparate information into a final coherent summary.",
        llm=llm,
    )

    reader = ConsoleReader()

    reader.write("Assistant  : ", "What location do you want to learn about?")
    for prompt in reader:
        await (
            workflow.run(
                inputs=[
                    AgentWorkflowInput(prompt="Provide a short history of the location.", context=prompt),
                    AgentWorkflowInput(
                        prompt="Provide a comprehensive weather summary for the location today.",
                        expected_output="Essential weather details such as chance of rain, temperature and wind. Only report information that is available.",  # noqa: E501
                    ),
                    AgentWorkflowInput(
                        prompt="Summarize the historical and weather data for the location.",
                        expected_output="A paragraph that describes the history of the location, followed by the current weather conditions.",  # noqa: E501
                    ),
                ]
            )
            .on(
                # Event Matcher -> match agent's 'success' events
                lambda event: isinstance(event.creator, ChatModel) and event.name == "success",
                # log data to the console
                lambda data, event: reader.write(
                    "Updated message content: "
                    + "".join(str([message.content[0] for message in data.value.messages]))
                    + "\n",
                    data,
                ),
                EmitterOptions(match_nested=True),
            )
            .on(
                "success",
                lambda data, event: reader.write(
                    f"->Step '{data.step}' has been completed with the following outcome:\n",
                    data.state.final_answer,
                ),
            )
        )
        reader.write("Assistant  : ", "What location do you want to learn about?")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except FrameworkError as e:
        traceback.print_exc()
        sys.exit(e.explain())
```

{/* <!-- embedme typescript/examples/workflows/multiAgents.ts --> */}
```ts TypeScript [expandable]
```

</CodeGroup>

## Examples

<CardGroup cols={2}>
  <Card title="Python" icon="python" href="https://github.com/i-am-bee/beeai-framework/tree/main/python/examples/agents">
    Explore reference agent implementations in Python
  </Card>
  <Card title="TypeScript" icon="typescript" href="https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples/agents">
    Explore reference agent implementations in TypeScript
  </Card>
</CardGroup>
